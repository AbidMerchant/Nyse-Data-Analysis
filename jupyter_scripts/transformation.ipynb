{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import functions as fn\n",
    "from pyspark.sql import Window\n",
    "\n",
    "from config import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "                .builder \\\n",
    "                .appName(\"Transformation\") \\\n",
    "                .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       2| 2019-07-01 00:51:04|  2019-07-01 00:51:33|              1|          .00|         1|                 N|         193|         193|           1|        2.5|  0.5|    0.5|      1.14|           0|                  0.3|        4.94|                   0|\n",
      "|       2| 2019-07-01 00:46:04|  2019-07-01 01:05:46|              1|         4.16|         1|                 N|         234|          25|           2|       16.5|  0.5|    0.5|         0|           0|                  0.3|        20.3|                 2.5|\n",
      "|       1| 2019-07-01 00:25:09|  2019-07-01 01:00:56|              1|        18.80|         2|                 N|         132|          42|           1|         52|    0|    0.5|     11.75|        6.12|                  0.3|       70.67|                   0|\n",
      "|       2| 2019-07-01 00:33:32|  2019-07-01 01:15:27|              1|        18.46|         2|                 N|         132|         142|           1|         52|    0|    0.5|     11.06|           0|                  0.3|       66.36|                 2.5|\n",
      "|       1| 2019-07-01 00:00:55|  2019-07-01 00:13:05|              0|         1.70|         1|                 N|         107|         114|           1|        9.5|    3|    0.5|         2|           0|                  0.3|        15.3|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df = spark.read.orc(\"hdfs://localhost:9000/mnt/data/source/dataset\")\n",
    "\n",
    "data_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(fileName):\n",
    "    data_df = spark.read.orc(hdfs_source+fileName)\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "# Removing nulls & extracting columns\n",
    "def data_preprocess(data_df):\n",
    "    data_df2 = data_df.where(\"vendorID is not null\")\n",
    "\n",
    "    data_df3 = data_df2.withColumn(\"hour\", fn.hour(\"tpep_pickup_datetime\")) \\\n",
    "                       .withColumn(\"week\", fn.weekofyear(\"tpep_pickup_datetime\")) \\\n",
    "                       .withColumn(\"month\", fn.month(\"tpep_pickup_datetime\")) \\\n",
    "                       .withColumn(\"year\", fn.year(\"tpep_pickup_datetime\"))\n",
    "    \n",
    "    return data_df3\n",
    "\n",
    "# Week level aggregation of sales of each vendor\n",
    "def weekly_sales_aggr(data_df):\n",
    "    week_df = data_df.groupBy(\"vendorID\", \"week\", \"year\") \\\n",
    "                      .agg(fn.round(fn.sum(\"total_amount\"), 2).alias(\"weekly_sales\"))\n",
    "\n",
    "    week_df.orderBy(\"vendorID\", \"year\", \"week\") \\\n",
    "           .write.orc(hdfs_output+\"weekly_sales_aggr\", mode=\"overwrite\")\n",
    "\n",
    "# Month level aggregation of sales of each vendor\n",
    "def monthly_sales_aggr(data_df):\n",
    "    month_df = data_df.groupBy(\"vendorID\", \"month\", \"year\") \\\n",
    "                       .agg(fn.round(fn.sum(\"total_amount\"), 2).alias(\"monthly_sales\"))\n",
    "\n",
    "    month_df.orderBy(\"vendorID\", \"year\", \"month\") \\\n",
    "            .write.orc(hdfs_output+\"monthly_sales_aggr\", mode=\"overwrite\")\n",
    "\n",
    "# Average amount of congestion surcharge each vendor charged in each month \n",
    "def avg_monthly_surcharge(data_df):\n",
    "    avg_surcharge_df = data_df.groupBy(\"vendorID\", \"month\", \"year\") \\\n",
    "                               .agg(fn.round(fn.avg(\"congestion_surcharge\"), 2).alias(\"avg_monthly_surcharge\"))\n",
    "\n",
    "    avg_surcharge_df.orderBy(\"vendorID\", \"year\", \"month\") \\\n",
    "                    .write.orc(hdfs_output+\"avg_monthly_surcharge\", mode=\"overwrite\")\n",
    "\n",
    "# Distribution of percentage of trips at each hour of day \n",
    "def trips_hourly(data_df):\n",
    "    total_count = data_df.count()\n",
    "\n",
    "    hour_df = data_df.groupBy(\"hour\") \\\n",
    "                      .agg(fn.round(100*(fn.count(\"*\")/total_count), 2).alias(\"percentage_trips_hourly\"))\n",
    "\n",
    "    hour_df.orderBy(\"hour\") \\\n",
    "           .write.orc(hdfs_output+\"trips_hourly\", mode=\"overwrite\")\n",
    "\n",
    "# Top 3 payment types users used in each month. Get payment method name from id from payments otc table in mysql\n",
    "def top_payments_monthly(data_df):\n",
    "    payment_otc = spark.read.jdbc(mysql_url,\"source.payment_otc\",\n",
    "                              properties={\"user\":mysql_un,\"password\":mysql_password})\n",
    "\n",
    "    payment_df = data_df.groupBy(\"payment_type\", \"month\", \"year\") \\\n",
    "                             .count()\n",
    "\n",
    "    window = Window.partitionBy(\"month\", \"year\") \\\n",
    "                   .orderBy(fn.desc(\"count\"))\n",
    "\n",
    "    payment_df2 = payment_df.withColumn(\"rank\", fn.rank().over(window))\n",
    "\n",
    "    payment_df3 = payment_df2.where(\"rank <= 3\") \\\n",
    "                             .select(\"month\", \"year\", \"payment_type\", \"count\")\n",
    "\n",
    "    payment_df4 = payment_df3.join(payment_otc, [\"payment_type\"])\\\n",
    "                             .select(\"month\", \"year\", \"payment_name\", \"count\")\n",
    "\n",
    "    payment_df4.orderBy(\"year\", \"month\") \\\n",
    "               .write.orc(hdfs_output+\"top_payments_monthly\", mode=\"overwrite\")\n",
    "\n",
    "# Distribution of payment type in each month\n",
    "def payment_dist_monthly(data_df):\n",
    "    total_count = data_df.count()\n",
    "\n",
    "    payment_dist_df = data_df.groupBy(\"payment_type\", \"month\", \"year\") \\\n",
    "                              .count()\n",
    "\n",
    "    payment_dist_df2 = payment_dist_df.withColumn(\"payment_type_prcnt_dist\",\n",
    "                                                    fn.round(100*(fn.col(\"count\")/total_count), 2))\\\n",
    "                                      .select(\"month\", \"year\", \"payment_type\", \"payment_type_prcnt_dist\")\n",
    "\n",
    "    payment_dist_df2.orderBy(\"year\", \"month\") \\\n",
    "                    .write.orc(hdfs_output+\"payment_dist_monthly\", mode=\"overwrite\")\n",
    "\n",
    "# Total passengers each Vendor served in each month\n",
    "def total_passengers_monthly(data_df):\n",
    "    passengers_df = data_df.groupBy(\"vendorID\", \"month\", \"year\") \\\n",
    "                            .agg(fn.sum(\"passenger_count\").cast(\"Integer\").alias(\"total_passengers_per_month\")) \\\n",
    "\n",
    "    passengers_df.orderBy(\"vendorID\", \"month\", \"year\") \\\n",
    "                 .write.orc(hdfs_output+\"total_passengers_monthly\", mode=\"overwrite\")\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    data_df = readData(\"dataset\")\n",
    "    \n",
    "    data_df_new = data_preprocess(data_df)\n",
    "    \n",
    "    # Transformation functions\n",
    "    weekly_sales_aggr(data_df_new)\n",
    "    monthly_sales_aggr(data_df_new)\n",
    "    avg_monthly_surcharge(data_df_new)\n",
    "    trips_hourly(data_df_new)\n",
    "    top_payments_monthly(data_df_new)\n",
    "    payment_dist_monthly(data_df_new)\n",
    "    total_passengers_monthly(data_df_new)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
